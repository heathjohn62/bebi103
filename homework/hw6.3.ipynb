{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits: \n",
    "Jared thought out most of the priors, and Jared and John discussed the approach to the problem.\n",
    "\n",
    "John did the rest of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.3: Analysis of FRAP data (40 pts)\n",
    "\n",
    "In [homework set 4](hw4.html#Problem-4.1:-Analysis-of-FRAP-data-(40-pts)), we began analyzing a FRAP experiment by Nate Goehring and corworkers. You performed image analysis to obtain the mean fluorescence of the bleach spot versus time. In this problem, you will use those data to obtain estimates for the diffusion coefficient $D$ and chemical rate constant $k_\\mathrm{off}$ for the PH-PLCd1/PIP2 complex.\n",
    "\n",
    "As a reminder, we are taking a simplified approach, but there is more sophisticated analysis we can do to get better estimates for the phenomenological coefficients. These are discussed in the [Goehring, et al. paper](http://dx.doi.org/10.1016/j.bpj.2010.08.033). Instead, we will use the the mean fluorescence of the bleached region, $I(t)$ to perform our analysis. As derived in their paper, \n",
    "\n",
    "\\begin{align}\n",
    "I_\\mathrm{norm}(t) \\equiv I(t)/I_0 &= \n",
    "f_f\\left(1 - f_b\\,\\frac{4 \\mathrm{e}^{-k_\\mathrm{off}t}}{d_x d_y}\\,\\psi_x(t)\\,\\psi_y(t)\\right),\\\\[1mm]\n",
    "\\text{where } \\psi_i(t) &= \\frac{d_i}{2}\\,\\mathrm{erf}\\left(\\frac{d_i}{\\sqrt{4Dt}}\\right)\n",
    "-\\sqrt{\\frac{D t}{\\pi}}\\left(1 - \\mathrm{e}^{-d_i^2/4Dt}\\right),\n",
    "\\end{align}\n",
    "\n",
    "where $d_x$ and $d_y$ are the extent of the photobleached box in the $x$- and $y$-directions, $f_b$ is the fraction of fluorophores that were bleached, $f_f$ is the fraction of total fluorescent species left after photobleaching, and $\\mathrm{erf}(x)$ is the [error function](http://en.wikipedia.org/wiki/Error_function). Here, $I_0$ is the mean fluorescence in the bleach spot before bleaching. Note that this function is defined such that the photobleaching event occurs at time $t = 0$.\n",
    "\n",
    "Your task in this problem is to develop a generative model and then to find estimates for the parameters of the model. We will revisit this problem again later in the course and build a hierarchical model. For this problem, consider each of the eight trials separately and use optimization to find the MAP parameter values for each trial and make a Gaussian approximation of the posterior to give an approximate 95% credible region.\n",
    "\n",
    "You should have acquired a data set of mean fluorescence versus time, and you should use that for your analysis. If you do not have that data set, you can download those generated in the [solutions](http://bebi103.caltech.edu/2018_protected/hw_solutions/hw4_solutions.html) [here](http://bebi103.caltech.edu/2018_protected/hw_solutions/hw_4.1_frap_image_processing_results.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import scipy.stats as st\n",
    "import statsmodels.tools.numdiff as smnd\n",
    "\n",
    "import bebi103\n",
    "\n",
    "import altair as alt\n",
    "import bokeh.plotting\n",
    "import bokeh.io\n",
    "from bokeh.palettes import all_palettes \n",
    "from bokeh.models import Legend, LegendItem\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by importing the data from assignment 4.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./FRAP_intensities.csv\").drop(columns = [\"Unnamed: 0\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has not been normalized, and I'd like to make the total intensity values in the viscinity of 1. The data came from a 10-bit camera, so the max pixel intensity is given by $2^{10}-1 = 1023$. Thus, I will divide all intensities by the area of the roi ($40^2$) and 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in range(0, 8):\n",
    "    name = \"FRAP_exp_%i\"%exp\n",
    "    df[name] = df[name].values / (1000 * 40 * 40)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photobleaching occurs at $t = 3.760$, but we want this time to be $t=0$. Thus, I will subtract 3.76 from all times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_(s)\"] = df[\"time_(s)\"].values - 3.76\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks good! \n",
    "\n",
    "Let's think carefully about our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of each datapoint is given by the theoretical prediction, plus some guassian error. Thus, we have the following:\n",
    "$$\\text{Likelihood} = \\text{Norm}(I(t), \\sigma).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, we now have six parameters to estimate: $I_0$, $f_b$, $f_f$, $k_\\mathrm{off}$, $D$, and $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I_0$ = Norm(1, .25): This is the value of the function prior to time point 0. It should be ~1, so accounting for the variation in measurement we will represent it as a Gaussian about 1, being careful to keep the distribution tight enough to not include negative values. \n",
    "\n",
    "\n",
    "$f_f$ = Beta(5.5, 1.4): This is the fraction of total fluorophores left after photobleaching, so it is the value of $I(0)/I_0$. Since is is between 0 and 1, this should be easily modeled as a beta distribution that leans heavily towards 1 (we expect that the bleached region is small in comparison to the cell). \n",
    "\n",
    "\n",
    "$f_b$ = Beta(10, 1.4): This is the fraction of fluorphores within the bleached region that were bleached. It must be between 0 and 1, so a beta distribution would make sense. Additionally, we expect the bleached region to be almost entirely bleached, so this should lean very heavily towards 1. \n",
    "\n",
    "$D$ = LogNorm(ln(1.7), .75): From the paper we expect the diffusion coefficient to be in the viscinity of 1.7 $\\frac{\\mu \\text{m}^2}{\\text{s}}$, but we wand a lot of variability, and also to keep it greater than zero. Thus, log-normal seemed like a reasionable choice. \n",
    "\n",
    "$k_\\mathrm{off}$ = LogNorm(ln(.12), .75): Also from the paper, we expect $k_\\mathrm{off}$ to be around .12 $\\text{s}^{-1}$, and we made it lognormal for the same reasons as we made $D$ lognormal. \n",
    "\n",
    "$\\sigma$ = Norm(.1, .1): We expect a variability in measurement of about .1, and to vary from that by about .1. \n",
    "\n",
    "Thankfully we know $d_x$ and $d_y$ exactly. \n",
    "$$d_x = d_y = 40 \\text{ pixels} * \\frac{.138 \\mu \\mathrm{m}}{1\\text{ pixel}} = 5.52 \\,\\mu\\text{m}$$\n",
    "\n",
    "Now I will code up the likelihood function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5.52 #Î¼m\n",
    "\n",
    "# Since dx = dy, I can make one psi function and re-use it\n",
    "def psi(params, t):\n",
    "    I_0, f_b, f_f, k_off, D, sigma = params\n",
    "    temp = d / 2 * scipy.special.erf(d / np.sqrt(4 * D * t)) \n",
    "    temp -= np.sqrt(D * t / np.pi) * (1 - np.exp(-d * d / (4 * D * t)))\n",
    "    return temp\n",
    "\n",
    "def I(params, t):\n",
    "    # Unpack parameters\n",
    "    I_0, f_b, f_f, k_off, D, sigma = params\n",
    "    # If the time is not yet at photobleaching, the model predicts intensity I_0\n",
    "    if t<0:\n",
    "        return I_0\n",
    "    # Otherwise return the post-photobleaching result\n",
    "    return I_0 * f_f * (1 - f_b * (4 * np.exp(-k_off * t) / (d * d) * np.power(psi(params, t), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to do a prior predictive check on our model before working with any data. This should be simple enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "time = df[\"time_(s)\"].values\n",
    "p = bokeh.plotting.Figure(x_axis_label = \"Time (s)\",\n",
    "                          y_axis_label = \"Fluorescence Intensity\")\n",
    "for sample in range(0, num_samples):\n",
    "    # Draw each parameter from its distribution\n",
    "    I_0 = 0.25 * np.random.randn() + 1\n",
    "    f_b = np.random.beta(10, 1.4)\n",
    "    f_f = np.random.beta(5.5, 1.4)\n",
    "    D = np.random.lognormal(mean = 1.7, sigma = .75)\n",
    "    k_off = np.random.lognormal(mean = .12, sigma = .75)\n",
    "    sigma = 0.1 * np.random.randn() + 0.1\n",
    "    \n",
    "    # Combine parameters into an array\n",
    "    params = [I_0, f_b, f_f, k_off, D, sigma]\n",
    "    \n",
    "    # Plot each model\n",
    "    model = np.zeros(len(time))\n",
    "    for i, t in zip(range(0, len(time)), time):\n",
    "        model[i] = I(params, t)\n",
    "    p.line(time, model, alpha = 0.3)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prior predictive check looks great! Let's turn our focus to finding MAP parameters. The first step is to define the negtive log posterior so that we can minimize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_post(params, intensities, time):\n",
    "    \"\"\"Negative log posterior for FRAP analysis.\"\"\"\n",
    "    # Make sure parameters are physical\n",
    "    if (params < 0).any():\n",
    "        return np.inf\n",
    "    \n",
    "    # Unpack params\n",
    "    I_0, f_b, f_f, k_off, D, sigma = params\n",
    "    \n",
    "    # Calculate the log likelihood\n",
    "    post = 0\n",
    "    for t, intensity in zip(time, intensities):\n",
    "        post += st.norm.logpdf(intensity, loc=I(params, t), scale=sigma)\n",
    "    \n",
    "    # Add the log priors for each parameter\n",
    "    # I_0\n",
    "    post += st.norm.logpdf(I_0, loc=1, scale=0.25)\n",
    "    # f_b\n",
    "    post += st.beta.logpdf(f_b, 10, 1.4)\n",
    "    # f_f\n",
    "    post += st.beta.logpdf(f_f, 5.5, 1.4)\n",
    "    # k_off\n",
    "    post += st.lognorm.logpdf(k_off, .75, scale = np.exp(.12))\n",
    "    # D\n",
    "    post += st.lognorm.logpdf(D, .75, scale = np.exp(1.7))\n",
    "    # sigma\n",
    "    post += st.norm.logpdf(sigma, loc = 0.1, scale = 0.1)\n",
    "    \n",
    "    return -1 * post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I just need to define initial parameters and run `scipy.optimize.minimize()` to find the MAP parameters that minimize the negative log posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial parameters\n",
    "params_0 = [1, .9, .9, .12, 1.7, .1]\n",
    "\n",
    "# Create Dataframe for storing MAP parameters\n",
    "cols = [\"Exp_num\", \"I_0\", \"f_b\", \"f_f\", \"k_off\", \"D\", \"sigma\"]\n",
    "df_params = pd.DataFrame(columns = cols)\n",
    "\n",
    "for exp_num in range(0, 8):\n",
    "    # Establish arguments for each experiment\n",
    "    args = (df[\"FRAP_exp_%i\"%exp_num].values, df[\"time_(s)\"].values)\n",
    "\n",
    "    # Compute the MAP\n",
    "    res = scipy.optimize.minimize(neg_log_post, \n",
    "                                  params_0,\n",
    "                                  args=args,\n",
    "                                  method='powell')\n",
    "    \n",
    "    parameters = np.append(np.array([exp_num], dtype = int), res.x)\n",
    "    df_params = df_params.append(pd.DataFrame(data=[parameters],columns = cols))\n",
    "df_params.index = np.ndarray.astype(df_params[\"Exp_num\"].values, int)\n",
    "df_params = df_params.drop(columns = [\"Exp_num\"])\n",
    "df_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will find and report 95% confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a log_posterior function because I need it for the hessian\n",
    "def log_post(params, intensities, time):\n",
    "    return -1 * neg_log_post(params, intensities, time)\n",
    "\n",
    "# Store covariance matrices\n",
    "covariances = [0]*8\n",
    "\n",
    "# Iterate through experiments and calculate hessians, covariances, and report\n",
    "for row, exp_num in zip(df_params.iterrows(), range(0, 8)):\n",
    "    # Extract parameters from row\n",
    "    row = row[1].values\n",
    "    \n",
    "    # Establish arguments for each experiment\n",
    "    args = (df[\"FRAP_exp_%i\"%exp_num].values, df[\"time_(s)\"].values)\n",
    "    \n",
    "    # Compute hessian\n",
    "    hess = smnd.approx_hess(row, log_post, args=args)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    cov = -np.linalg.inv(hess)\n",
    "    covariances[exp_num] = cov\n",
    "    \n",
    "    # Report parameters within 95% confidence\n",
    "    print(\"Experiment #%i\" % exp_num)\n",
    "    for name, param, i in zip(cols[1:], row, range(0, len(row))):\n",
    "        print(name + \": %.4f Â± %.4f\"%(param, 2 * np.sqrt(cov[i,i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, printed data is not as good as graphed data. I will report the credible regions for the diffusion coefficients and chemical rate constants. I will start this by making dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_D = [0]*len(covariances)\n",
    "int_k_off = [0]*len(covariances)\n",
    "for cov, i in zip(covariances, range(0, len(covariances))):\n",
    "    k_off = df_params[\"k_off\"].values[i]\n",
    "    D = df_params[\"D\"].values[i]\n",
    "    int_D[i] = np.array([D - np.sqrt(cov[4,4]), D + np.sqrt(cov[4,4])])\n",
    "    int_k_off[i] = np.array([k_off - np.sqrt(cov[3,3]), k_off + np.sqrt(cov[3,3])])\n",
    "    \n",
    "k = bokeh.plotting.Figure(height = 200,\n",
    "                          width = 400,\n",
    "                          x_axis_label = \"k_off\",\n",
    "                          y_axis_label = \"Experiment #\",\n",
    "                          title = \"95% Confidence Intervals for k_off\")\n",
    "for i in range(0, 8):\n",
    "    k.line(int_k_off[i], [i, i], line_width = 3)\n",
    "bokeh.io.show(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = bokeh.plotting.Figure(height = 200,\n",
    "                          width = 400,\n",
    "                          x_axis_label = \"D\",\n",
    "                          y_axis_label = \"Experiment #\",\n",
    "                          title = \"95% Confidence Intervals for diffusion coefficient\")\n",
    "for i in range(0, 8):\n",
    "    D.line(int_D[i], [i, i], line_width = 3)\n",
    "bokeh.io.show(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is especially striking that is so little overlap between diffusion coeficients and k_off.\n",
    "\n",
    "To ensure the validity of these results, I want to make one last plot that compares the actual data with its respective MAP approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deals with coloring of different lines\n",
    "colors = all_palettes['Viridis'][8]\n",
    "\n",
    "first = True # used to only show the first plot. \n",
    "\n",
    "p = bokeh.plotting.Figure(width = 800, \n",
    "                          height = 500,\n",
    "                          title = \"Actual vs. Modeled Fluorescence Intensity\",\n",
    "                          x_axis_label = \"Time (s)\")\n",
    "\n",
    "\n",
    "for row, exp_num in zip(df_params.iterrows(), range(0, 8)):\n",
    "    # Extract parameters from row\n",
    "    row = row[1].values\n",
    "    name = \"FRAP_exp_%i\"%exp_num\n",
    "    \n",
    "    # Calculate values for theoretical model with MAP parameters\n",
    "    model = np.zeros(len(df[\"time_(s)\"].values))\n",
    "    for t , i in zip(df[\"time_(s)\"].values, range(0, len(model))):\n",
    "        model[i] = I(row, t)\n",
    "        \n",
    "    p.line(df['time_(s)'].values,\n",
    "           df[name].values, \n",
    "           color=colors[exp_num], \n",
    "           visible = first, \n",
    "           legend = \"Experiment %i\"%exp_num)\n",
    "    \n",
    "    p.line(df['time_(s)'].values,\n",
    "           model, \n",
    "           color='black', \n",
    "           visible = first, \n",
    "           legend = \"Experiment %i\"%exp_num)\n",
    "    \n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.legend.location = \"bottom_right\"\n",
    "    first = False # used to only show the first plot. \n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "**Grade: 39/40**\n",
    "\n",
    "- A formatting note: typically you would write âparameter ~ distributionâ rather than âparameter = distributionâ\n",
    "- It isnât really fair game to construct priors for $D$ and $k_{off}$ based on the results from the paper in which the data have already been analyzed.  You should be constructing priors based on general previous biological knowledgeâ¦ (-0.5 pts)\n",
    "- Your prior predictive checks look decent overall, and they were plotted well.  However, the recovery times seem a bit fast (< 3 seconds), and I would have lowered where my priors for $D$ and/or $k_{off}$ were centered after seeing those curves. (-0.5 pts)?\n",
    "- Your optimization took much longer to converge than expected, and Iâm not sure why.  But it turned out ok I guess!\n",
    "- In constructing your 95% credible regions, 95% is actually 1.96 times the standard deviation, not 2.  Itâs very close, but we were specific in asking for 95% rather than two standard deviations or 95.45%.  I wonât take off points, but be careful in the future.\n",
    "- I share your concern regarding the lack of overlap between the 95% confidence regions on your $D$ and $k_{off}$ parameter estimates.  Some of them are OK, but we saw a bit more overlap in our parameter estimates, despite our parameter values being of similar order of magnitude to yours.  Definitely within the realm of possibility thoughâ¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
