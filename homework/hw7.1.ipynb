{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BE/Bi 103 Homework 7\n",
    "###  Credits: \n",
    "A) John: mh_step() function\n",
    "\n",
    "Jared: mh_sampler() function\n",
    "\n",
    "B) John"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7.1: Writing your own MCMC sampler (60 pts)\n",
    "\n",
    "**a)** Write your own MCMC sampler that employs a Metropolis-Hastings algorithm to sample continuous parameters (i.e., it does not need to handle discrete parameters) that uses a Gaussian proposal distribution. Since you are sampling multiple parameters, your proposal distribution will be multi-dimensional. You can use a Gaussian proposal distribution with a diagonal covariance. In other words, you generate a proposal for each variable in the posterior independently.\n",
    "\n",
    "You can organize your code how you like, but here is a suggestion.\n",
    "\n",
    "* Write a function that takes (or rejects) a Metropolis-Hastings step. It should look something like the below (obviously where it does something instead of `pass`ing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_step(x, logpost, logpost_current, sigma, args=()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_variables,)\n",
    "        The present location of the walker in parameter space.\n",
    "    logpost : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `logpost(x, *args)`.\n",
    "    logpost_current : float\n",
    "        The current value of the log posterior.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `logpost()` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : ndarray, shape (n_variables,)\n",
    "        The position of the walker after the Metropolis-Hastings\n",
    "        step. If no step is taken, returns the inputted `x`.\n",
    "    \"\"\"\n",
    "    # Turn sigma into a diagonal matrix so that it can be used as\n",
    "    # the covariance matrix for numpy's multivariate guassian function\n",
    "    cov = np.diag(sigma * sigma) # squared because variance is std. squared\n",
    "    \n",
    "    # Now I will choose the next step, x-prime\n",
    "    xp = np.random.multivariate_normal(x, cov, 1)[0]\n",
    "    \n",
    "    # Compute the log-posterior at x-prime\n",
    "    logpost_xp = logpost(xp, *args)\n",
    "    \n",
    "    # Calculate metropolis ratio\n",
    "    r = np.exp(logpost_xp - logpost_current)\n",
    "    \n",
    "    if (r >= 1):\n",
    "        return xp\n",
    "    elif(np.random.rand() < r):\n",
    "        return xp\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write another function that calls that function over and over again to do the sampling. It should look something like the below. (Note that I am using `n_burn` as opposed to `n_warmup`, because you are just going to step as you normally would, and then \"burn\" the samples. This is in contrast to Stan, which tunes the stepping procedure duing the warm-up phase.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_sample(logpost, x0, sigma, args=(), n_burn=1000, n_steps=1000,\n",
    "              variable_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    logpost : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `logpost(x, *args)`.\n",
    "    x0 : ndarray, shape (n_variables,)\n",
    "        The starting location of a walker in parameter space.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `logpost()` function.\n",
    "    n_burn : int, default 1000\n",
    "        Number of burn-in steps.\n",
    "    n_steps : int, default 1000\n",
    "        Number of steps to take after burn-in.\n",
    "    variable_names : list, length n_variables\n",
    "        List of names of variables. If None, then variable names\n",
    "        are sequential integers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : DataFrame\n",
    "        The first `n_variables` columns contain the samples.\n",
    "        Additionally, column 'lnprob' has the log posterior value\n",
    "        at each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define variable names\n",
    "    if variable_names == None:\n",
    "        variable_names = list(range(len(x0)))\n",
    "    \n",
    "    # Define starting points\n",
    "    x = x0\n",
    "    logpost_current = logpost(x, *args)\n",
    "        \n",
    "    # Burn step    \n",
    "    for c in range(n_burn):\n",
    "        x = mh_step(x, logpost, logpost_current, sigma, args=args)\n",
    "        logpost_current = logpost(x, *args)\n",
    "    \n",
    "    \n",
    "    # Make arrays to take data\n",
    "    samples = [0]*n_steps\n",
    "    logpost_values = np.empty([len(range(n_steps))])\n",
    "    \n",
    "    # Do the sampling\n",
    "    for k in range(n_steps):\n",
    "        x = mh_step(x, logpost, logpost_current, sigma, args=args)\n",
    "        logpost_current = logpost(x, *args)\n",
    "        samples[k] = x\n",
    "        logpost_values[k] = logpost_current\n",
    "        \n",
    "    \n",
    "    # Convert arrays to dataframe    \n",
    "    df =  pd.DataFrame(data=samples, columns = variable_names)\n",
    "    df[\"lnprob\"] = logpost_values\n",
    "        \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** To test your code, we will get samples out of a known distribution. We will use a bivariate Gaussian with a mean of $\\boldsymbol{\\mu} = (10, 20)$ and covariance matrix of \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\sigma} = \\begin{pmatrix}\n",
    "4 & -2 \\\\\n",
    "-2 & 6\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "I have written the function to be unnormalized and JITted with [numba](https://numba.pydata.org) for optimal speed.\n",
    "\n",
    "Do not be confused: In this test function we are sampling $\\mathbf{x}$ out of $P(\\mathbf{x}\\mid \\boldsymbol{\\mu},\\boldsymbol{\\sigma})$. This is not sampling a posterior; it's just a test for your code. You will pass `log_test_distribution` as the `log_post` argument in the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.array([10.0, 20])\n",
    "cov = np.array([[4, -2],[-2, 6]])\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def log_test_distribution(x, mu, inv_cov):\n",
    "    \"\"\"\n",
    "    Unnormalized log posterior of a multivariate Gaussian.\n",
    "    \"\"\"\n",
    "    return -np.dot((x-mu), np.dot(inv_cov, (x-mu))) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the means and covariance (using `np.cov()`) of your samples, you should come close to the inputed means and covariance. You might also want to plot your samples using `bebi103.viz.corner()` to make sure everything makes sense.\n",
    "\n",
    "<br />\n",
    "\n",
    "You may want to add in some logic to your Metropolis-Hastings sampler to enable *tuning*. This is the process of automatically adjusting the $\\sigma$ in the proposal distribution such that the acceptance rate is desirable. The target acceptance rate is about 0.4. The developers of [PyMC3](https://github.com/pymc-devs/pymc3) use the scheme below, which is reasonable.\n",
    "\n",
    "|Acceptance rate|Standard deviation adaptation|\n",
    "|:---:|:-------------------:|\n",
    "| < 0.001        |$\\times$ 0.1|\n",
    "|< 0.05         |$\\times$ 0.5|\n",
    "|< 0.2          |$\\times$ 0.9|\n",
    "|> 0.5          |$\\times$ 1.1|\n",
    "|> 0.75         |$\\times$ 2|\n",
    "|> 0.95         |$\\times$ 10|\n",
    "\n",
    "Be sure to test your code to demonstrate that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>lnprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.736044</td>\n",
       "      <td>19.771434</td>\n",
       "      <td>-0.021708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.144924</td>\n",
       "      <td>20.282549</td>\n",
       "      <td>-0.015229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.962231</td>\n",
       "      <td>20.007777</td>\n",
       "      <td>-0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.759013</td>\n",
       "      <td>19.582742</td>\n",
       "      <td>-0.036177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.682013</td>\n",
       "      <td>20.237301</td>\n",
       "      <td>-0.013253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var_1      var_2    lnprob\n",
       "0   9.736044  19.771434 -0.021708\n",
       "1  10.144924  20.282549 -0.015229\n",
       "2   9.962231  20.007777 -0.000191\n",
       "3   9.759013  19.582742 -0.036177\n",
       "4   9.682013  20.237301 -0.013253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial guess for MCMC\n",
    "x_0 = np.array([1,1])\n",
    "# Standard Deviations for proposal dist.\n",
    "s = np.array([1,1])\n",
    "df_mcmc_test = mh_sample(log_test_distribution, \n",
    "                         x_0, \n",
    "                         s, \n",
    "                         args = (mu, inv_cov), \n",
    "                         variable_names = [\"var_1\", \"var_2\"],\n",
    "                         n_steps = 4000)\n",
    "df_mcmc_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the sampling worked! Let's see how the parameters compare to the real distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC mean of var_1: 9.9744    Actual mean of var1: 10.0\n",
      "MCMC mean of var_2: 20.0402    Actual mean of var2: 20.0\n",
      "\n",
      "MCMC covariance matrix: 4.3028 -2.2645 \n",
      "                       -2.2645 6.4868\n",
      "\n",
      "Actual covariance matrix: 4.0000 -2.0000 \n",
      "                         -2.0000 6.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"MCMC mean of var_1: %.4f    Actual mean of var1: 10.0\" % \n",
    "      np.mean(df_mcmc_test[\"var_1\"].values))\n",
    "print(\"MCMC mean of var_2: %.4f    Actual mean of var2: 20.0\" % \n",
    "      np.mean(df_mcmc_test[\"var_2\"].values))\n",
    "covariance = np.cov(df_mcmc_test[\"var_1\"].values, df_mcmc_test[\"var_2\"].values)\n",
    "print(\"\")\n",
    "print(\n",
    "\"MCMC covariance matrix: %.4f %.4f \\n                       %.4f %.4f\" \n",
    "    %(covariance[0][0],\n",
    "      covariance[0][1],\n",
    "      covariance[1][0],\n",
    "      covariance[1][1]))\n",
    "print(\"\")\n",
    "print(\n",
    "\"Actual covariance matrix: %.4f %.4f \\n                         %.4f %.4f\"\n",
    "    %(cov[0][0],\n",
    "      cov[0][1],\n",
    "      cov[1][0],\n",
    "      cov[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are pretty variable when there is a limited number of samples. Perhaps this can be improved via acceptance rate optimization. \n",
    "\n",
    "We are interested in calculating the acceptance rate of moves in our MCMC sampler. I will write a function for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acceptance_rate(output_df):\n",
    "    \"\"\"Determines the acceptance rate of an MCMC sampler within the passed\n",
    "    dataframe containing MCMC samples. \"\"\"\n",
    "    num_rejections = 0\n",
    "    last_row = np.zeros(len(output_df.loc[0].values))\n",
    "    for row in output_df.iterrows():\n",
    "        row = row[1].values\n",
    "        if (row == last_row).all():\n",
    "            num_rejections += 1\n",
    "        last_row = row\n",
    "    return (len(output_df.index) - num_rejections) / len(output_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do a quick check to make sure that this function is sensible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_acceptance_rate(df_mcmc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are being slightly too forgiving in the number of steps we are accepting! I will now write a small helper function that will determine the standard deviation correction, essentially from a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_std_dev_correction(acceptance_rate):\n",
    "    \"\"\"Returns the necessary multiplicative correction to the standard deviation\n",
    "    of the proposal distribution given an acceptance rate for the metropolis-\n",
    "    hastings MCMC sampler. \"\"\"\n",
    "    if acceptance_rate < .001:\n",
    "        return 0.1\n",
    "    elif acceptance_rate < 0.05:\n",
    "        return 0.5\n",
    "    elif acceptance_rate < 0.2:\n",
    "        return 0.9\n",
    "    elif acceptance_rate < 0.5:\n",
    "        return 1\n",
    "    elif acceptance_rate < 0.75:\n",
    "        return 1.1\n",
    "    elif acceptance_rate < 0.95:\n",
    "        return 2\n",
    "    else:\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to re-write my sampling function to ensure that the acceptance rate is valid. I will do a trial of sampling after the burn step to ensure there is a good acceptance rate, and repeat trials until the acceptance rate is acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_sample_optimize_acceptance(logpost, \n",
    "                                  x0, \n",
    "                                  sigma, \n",
    "                                  args=(), \n",
    "                                  n_burn=1000, \n",
    "                                  n_steps=1000, \n",
    "                                  n_opt_acc = 1000,\n",
    "                                  variable_names=None):\n",
    "    \"\"\"New sampling function that includes optimization of sigma\n",
    "    with respect to the acceptance rate. n_opt_acc indicates the number\n",
    "    of steps that should be used to optimize, but all other parameters\n",
    "    are identical to those used in mh_sample()\"\"\"\n",
    "    \n",
    "    # Define variable names\n",
    "    if variable_names == None:\n",
    "        variable_names = list(range(len(x0)))\n",
    "    \n",
    "    # Define starting points\n",
    "    x = x0\n",
    "    logpost_current = logpost(x, *args)\n",
    "        \n",
    "    # Burn step    \n",
    "    for c in range(n_burn):\n",
    "        x = mh_step(x, logpost, logpost_current, sigma, args=args)\n",
    "        logpost_current = logpost(x, *args)\n",
    "    \n",
    "    # Starting optimization!!\n",
    "    iterations = 0\n",
    "    while(True):\n",
    "        # Make arrays to take optimization data\n",
    "        samples_opt = [0]*n_opt_acc\n",
    "        logpost_values_opt = np.empty([len(range(n_opt_acc))])\n",
    "\n",
    "        # Do the sampling\n",
    "        for k in range(n_opt_acc):\n",
    "            x = mh_step(x, logpost, logpost_current, sigma, args=args)\n",
    "            logpost_current = logpost(x, *args)\n",
    "            samples_opt[k] = x\n",
    "            logpost_values_opt[k] = logpost_current\n",
    "\n",
    "        # Convert arrays to dataframe    \n",
    "        df_opt =  pd.DataFrame(data=samples_opt, columns = variable_names)\n",
    "        df_opt[\"lnprob\"] = logpost_values_opt\n",
    "        \n",
    "        # Calculate acceptance rate and optimize sigma\n",
    "        rate = calc_acceptance_rate(df_opt)\n",
    "        opt_factor = find_std_dev_correction(rate)\n",
    "        sigma *= opt_factor\n",
    "        \n",
    "        # If the acceptance rate was way off, repeat this process again. \n",
    "        # Otherwise, we can exit the loop\n",
    "        if (opt_factor == 1):\n",
    "            break\n",
    "            \n",
    "        # This program has the potential for an infinite loop. I don't want\n",
    "        # this to happen, so if this iterates 10 times I'll print a warning\n",
    "        # statement and stop optimizing. \n",
    "        iterations += 1\n",
    "        if iterations >= 10:\n",
    "            print(\"WARNING: Acceptance rate did not converge!\")\n",
    "            break\n",
    "    \n",
    "    # Make arrays to take data\n",
    "    samples = [0]*n_steps\n",
    "    logpost_values = np.empty([len(range(n_steps))])\n",
    "    \n",
    "    # Do the sampling\n",
    "    for k in range(n_steps):\n",
    "        x = mh_step(x, logpost, logpost_current, sigma, args=args)\n",
    "        logpost_current = logpost(x, *args)\n",
    "        samples[k] = x\n",
    "        logpost_values[k] = logpost_current\n",
    "        \n",
    "    \n",
    "    # Convert arrays to dataframe    \n",
    "    df =  pd.DataFrame(data=samples, columns = variable_names)\n",
    "    df[\"lnprob\"] = logpost_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run our previous check and see if the results are improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC mean of var_1: 10.1283    Actual mean of var1: 10.0\n",
      "MCMC mean of var_2: 19.9409    Actual mean of var2: 20.0\n",
      "\n",
      "MCMC covariance matrix: 4.0311 -2.0677 \n",
      "                       -2.0677 5.8159\n",
      "\n",
      "Actual covariance matrix: 4.0000 -2.0000 \n",
      "                         -2.0000 6.0000\n",
      "Acceptance Rate: 0.539\n"
     ]
    }
   ],
   "source": [
    "# Initial guess for MCMC\n",
    "x_0 = np.array([1,1])\n",
    "# Standard Deviations for proposal dist.\n",
    "s = np.array([1.,1.])\n",
    "df_mcmc_opt  = mh_sample_optimize_acceptance(log_test_distribution, \n",
    "                                             x_0, \n",
    "                                             s, \n",
    "                                             args = (mu, inv_cov), \n",
    "                                             variable_names = [\"var_1\", \"var_2\"],\n",
    "                                             n_steps = 4000)\n",
    "\n",
    "print(\"MCMC mean of var_1: %.4f    Actual mean of var1: 10.0\" % \n",
    "      np.mean(df_mcmc_opt[\"var_1\"].values))\n",
    "print(\"MCMC mean of var_2: %.4f    Actual mean of var2: 20.0\" % \n",
    "      np.mean(df_mcmc_opt[\"var_2\"].values))\n",
    "covariance = np.cov(df_mcmc_opt[\"var_1\"].values, df_mcmc_opt[\"var_2\"].values)\n",
    "print(\"\")\n",
    "print(\n",
    "\"MCMC covariance matrix: %.4f %.4f \\n                       %.4f %.4f\" \n",
    "    %(covariance[0][0],\n",
    "      covariance[0][1],\n",
    "      covariance[1][0],\n",
    "      covariance[1][1]))\n",
    "print(\"\")\n",
    "print(\n",
    "\"Actual covariance matrix: %.4f %.4f \\n                         %.4f %.4f\"\n",
    "    %(cov[0][0],\n",
    "      cov[0][1],\n",
    "      cov[1][0],\n",
    "      cov[1][1]))\n",
    "print(\"Acceptance Rate: %.3f\" % calc_acceptance_rate(df_mcmc_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little hard to see whether the results have actually improved, but the acceptance rate has certainly been optimized. Our MCMC sampler works as predicted. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
