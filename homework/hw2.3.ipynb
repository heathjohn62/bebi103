{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 (Bayes's theorem as a model for learning: 10 pts)\n",
    "\n",
    "Say you did an experiment to investigate a model with parameters $\\theta$ and acquired a data set $y_1$. Recall that in building a model for the experiment, you will have specified the likelihood, $f(y_1\\mid \\theta)$ and prior, $g(\\theta)$. The result of your experiment and modeling is the posterior, $g(\\theta\\mid y_1)$. Now, say you did a second experiment under the same model and acquired a data set $y_2$. Show that the posterior distribution you obtain by using the result of the first experiment as prior information for the second is the same as what you would obtain by pooling the results of the two experiments together into a single data set.\n",
    "\n",
    "In this way, we see how we learn more by doing more and more experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (This code enables equation numbering for this markdown cell.)\n",
    "[//]: # (Equation numbering is not yet fully integratedinto jupyter, )\n",
    "[//]: # (and this is a simple workaround. Numbering only works within )\n",
    "[//]: # (the same markdown cell, which is why this cell looks like an )\n",
    "[//]: # (entire latex document. )\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "</script>\n",
    "\n",
    "### Solution\n",
    "The posterior distribution after the first experiment is given by $g(\\theta\\mid y_1)$, and the distribution obtained from pooling data $y_1$ and $y_2$ is given by $g(\\theta\\mid y_1, y_2)$. Thus, the object of the proof is to demonstrate that if we use the use the posterior distribution from the first experiment as prior for the second, meaning if:\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_2) = \\frac{f(y_2 \\mid \\theta) g(\\theta)}{f(y_2)} = \\frac{f(y_2 \\mid \\theta) g(\\theta \\mid y_1)}{f(y_2)}\n",
    "\\label{eq:1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "then the resulting distribution is the same as if the data were simply pooled, meaning:\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_2) = g(\\theta \\mid y_1, y_2)\n",
    "\\label{eq:2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can use Bayes' Theorem to expand $g(\\theta \\mid y_1)$. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_1) = \\frac{f(y_1 \\mid \\theta)g(\\theta)}{f(y_1)}\n",
    "\\label{eq:3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now we can substitute equation $\\ref{eq:3}$ into equation $\\ref{eq:1}$. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_2) = \\frac{f(y_2 \\mid \\theta) f(y_1 \\mid \\theta) g(\\theta)}{f(y_1) f(y_2)}\n",
    "\\label{eq:4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The next portion of the proof requires that we assume that our data, $y_1$ and $y_2$, are completely independent of each other. This is an assumption that will not always be valid; a simple example would be a behavioral experiment in which an animal was frightened from the first experiment, and thus reacted differently during the second experiment. In this case, collecting more data might not actually permit us to learn more about the natural behavior of that animal, and the conclusions of this proof would not hold. This is worthy of note, and should have been included in the problem statement. However, for independent events $x_1$ and $x_2$, we know that $P(x_1, x_2) = P(x_1)P(x_2)$. Applying this knowledge to equation $\\ref{eq:4}$ gives the following:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_2) = \\frac{f(y_1, y_2 \\mid \\theta) g(\\theta)}{f(y_1, y_2)}\n",
    "\\label{eq:5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We are almost there! Now we notice that applying Bayes' theorem to the posterior distribution from the pooled data yields the same result:\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_1, y_2) = \\frac{f(y_1, y_2 \\mid \\theta)g(\\theta)}{f(y_1, y_2)}\n",
    "\\label{eq:6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Finally, by equating lines $\\ref{eq:5}$ and $\\ref{eq:6}$, we complete the proof. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(\\theta \\mid y_2) = g(\\theta \\mid y_1, y_2)\n",
    "\\label{eq:7}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\\text{QED}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
