{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: exercise\n",
    "\n",
    "(c) 2018 Justin Bois. With the exception of pasted graphics, where the source is noted, this work is licensed under a [Creative Commons Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). All code contained herein is licensed under an [MIT license](https://opensource.org/licenses/MIT).\n",
    "\n",
    "This document was prepared at [Caltech](http://www.caltech.edu) with financial support from the [Donna and Benjamin M. Rosen Bioengineering Center](http://rosen.caltech.edu).\n",
    "\n",
    "<img src=\"./caltech_rosen.png\">\n",
    "\n",
    "*This tutorial exercise was generated from an Jupyter notebook.  You can download the notebook [here](t6_exercise.ipynb). Use this downloaded Jupyter notebook to fill out your responses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "What is the difference between the prior distribution and posterior distribution?\n",
    "\n",
    "The prior distribution, given by $\\mathbb{P}(\\theta)$, is our initial level of belief that the true probability distribution from which we can draw evidence is parametrized by $\\theta$. The posterior distribution, given by $\\mathbb{P}(\\theta | x)$, is our revised belief that the true probability distribution is parametrized by $\\theta$ given the evidence that has been observed, $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Why are prior predictive checks an important step in the process of building a generative model?\n",
    "\n",
    "A generative model will generate data, and if the generation process is complex, it might be difficult to tell whether the end result of the generation process is capable of being a reasonable model for the real data. Thus, prior-predictive checks are used to ensure that generated data is consistent with our intuition for the physical problem being studied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Why is it necessary to compute summaries of the posterior? In other words, why can't we just write down the posterior (which is what the process of building a model is anyway) and be done with it?\n",
    "\n",
    "Often we want to characterize the posterior so that we have some intuition about the types of values it will predict, and this can be difficult if the posterior is time consuming to calculate or work with analytically. Approximating the posterior within a certain region can give simple insights into how the posterior behaves, such as finding error bars on parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "When performing parameter estimation by optimization, after we find the MAP, why do we locally approximate the posterior near the MAP as a (possibly multivariate) Gaussian?\n",
    "\n",
    "Since we are focusing on a maximum, it is reasonable to believe that the posterior appears gaussian with the mean of a variables at the maximum. Additionally, due to the friendly nature of the guassian, the covariance matrix may then be computed easily via matrix inversion operations that are guaranteed to work, and this will give reasonable error bounds.\n",
    "\n",
    "The guassian is likely to be a good fit, and can be easily used to calculate error. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
