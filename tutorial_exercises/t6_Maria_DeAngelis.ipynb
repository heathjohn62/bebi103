{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: exercise\n",
    "\n",
    "(c) 2018 Justin Bois. With the exception of pasted graphics, where the source is noted, this work is licensed under a [Creative Commons Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). All code contained herein is licensed under an [MIT license](https://opensource.org/licenses/MIT).\n",
    "\n",
    "This document was prepared at [Caltech](http://www.caltech.edu) with financial support from the [Donna and Benjamin M. Rosen Bioengineering Center](http://rosen.caltech.edu).\n",
    "\n",
    "*This tutorial exercise was generated from an Jupyter notebook.  You can download the notebook [here](t6_exercise.ipynb). Use this downloaded Jupyter notebook to fill out your responses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "What is the difference between the prior distribution and posterior distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution contains the informed guess of the experimenter of a parameter.\n",
    "\n",
    "The prior distribution is updated using Bayes' theorem to get a posterior distribution (represents updated beliefs of the parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Why are prior predictive checks an important step in the process of building a generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They help us check that the data generation process we have defined actually produces data that obeys physical constraints and matches our beliefs. It's helpful in checking if the model uses the data we have fed into it (so it's a bit of circular reasoning, but it's better than nothing!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Why is it necessary to compute summaries of the posterior? In other words, why can't we just write down the posterior (which is what the process of building a model is anyway) and be done with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a bit confused on this. Tutorial 6b outlined a bunch of different ways to summarize it, but didn't give much justification. I guess we want to summarize it to make it easier to use the results. Plots are nice to look at, but being able to summarize with a few numbers makes it much easier to write programs to do analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "When performing parameter estimation by optimization, after we find the MAP, why do we locally approximate the posterior near the MAP as a (possibly multivariate) Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP value doesn't give us any explicit information about the posterior probability density function, which is what we are trying to model. The Gaussian estimation can help us find an error bar for these parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
